Answer to Q1:

Trim 350 yields the larger optimal value of k at k = 16, compared to Trim 65 AMG, with k = 6. We believe the answer to why has to do with the bias-variance tradeoff. Using a higher k means we have much lower variance, as we take into account more of the points. But using a higher k means that we take into account points that may be far away from the x value, causing the prediction to shift away from the true value and creating bias. In other words, using a higher k causes underfitting in the model. Similarly, using a smaller k creates the opposite problem: while we know we will achieve closer to the true value because we only use the values of the data points around the x value, we are more prone to creating a model that can't generalize because it is trained too specifically to the training set. In other words, the curve will overfit the data. In this specific case, we believe that Trim 350 has the higher k because it requires a model that is simpler in nature as the data is far more "clumped" together than Trim 65 AMG's data is. Since it is a lot more dense, the bias is inherently going to be a lot lower, as more of the data points are going to be closer to the true value than would be the case in an average model. Thus, we boost the k-value a little bit to adjust for the decreased bias, and in turn, increased variance. 